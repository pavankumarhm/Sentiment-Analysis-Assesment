{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a0cd07",
   "metadata": {},
   "source": [
    "# Text Sentiment Analysis of IMDB Movie reviews using NLP (Word2Vec and RNN) for MYM Intern Assesment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca3e39",
   "metadata": {},
   "source": [
    "## Assesment Objectives: \n",
    "### - Preprocessing the data\n",
    "### - Converting Text(words) to Vectors using word2vec \n",
    "### - Using the word representations given by word2vec to feed a RNN and training the model\n",
    "### - Testing the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84781652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim==4.2.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep gensim ##Checking the version of Gensim - Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e2dbaa",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b25eb77",
   "metadata": {},
   "source": [
    "### Starting with 20% of the sentences from TensorFlow Datasets of IMDB reviews to check the RAM compatibility of the PC to train the model faster by splitting the datasets as X_train, y_train, X_test and y_test.\n",
    "### Then preprocessing the textual data to create input features for a natural language processing (NLP) model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2079b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "    \n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "      assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "        \n",
    "      len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "      train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "  \n",
    "      len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "      test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "    \n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f0fbe90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc1438",
   "metadata": {},
   "source": [
    "## First, training a word2vec model (with the arguments that we want) on your training sentence. Store it into the `word2vec` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f5c2e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(sentences=X_train, vector_size=60, min_count=10, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "62a835d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            embedded_sentence.append(word2vec.wv[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed = embedding(word2vec, X_train)\n",
    "X_test_embed = embedding(word2vec, X_test)\n",
    "\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dca840",
   "metadata": {},
   "source": [
    "### It's a good practice to check check the following for `X_train_pad` and `X_test_pad`:\n",
    "#### - they are numpy arrays\n",
    "#### - they are 3-dimensional\n",
    "#### - the last dimension is of the size of your word2vec embedding space (you can get it with `word2vec.wv.vector_size`\\\\\n",
    "#### - the first dimension is of the size of your `X_train` and `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d4770855",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X in [X_train_pad, X_test_pad]:\n",
    "    assert type(X) == np.ndarray\n",
    "    assert X.shape[-1] == word2vec.wv.vector_size\n",
    "\n",
    "\n",
    "assert X_train_pad.shape[0] == len(X_train)\n",
    "assert X_test_pad.shape[0] == len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30ae5fe",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3b477d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels in train set {0: 2474, 1: 2526}\n",
      "Baseline accuracy:  0.499\n"
     ]
    }
   ],
   "source": [
    "# It is always good to have a very simple model to test your own model against\n",
    "# Baseline accuracy can be to predict the label that is the most present in `y_train`.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "counts = dict(zip(unique, counts))\n",
    "print('Number of labels in train set', counts)\n",
    "\n",
    "y_pred = 0 if counts[0] > counts[1] else 1\n",
    "\n",
    "print('Baseline accuracy: ', accuracy_score(y_test, [y_pred]*len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecccfe0",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "523cd8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# writing a RNN model with Masking, LSTM and Dense layers.\n",
    "\n",
    "def init_model():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Masking())\n",
    "    model.add(layers.LSTM(20, activation='tanh'))\n",
    "    model.add(layers.Dense(15, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', #compiling the model with rmsprop optimizer\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5317ce64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "110/110 [==============================] - 7s 37ms/step - loss: 0.6849 - accuracy: 0.5554 - val_loss: 0.6843 - val_accuracy: 0.5653\n",
      "Epoch 2/100\n",
      "110/110 [==============================] - 3s 32ms/step - loss: 0.6465 - accuracy: 0.6311 - val_loss: 0.6471 - val_accuracy: 0.6273\n",
      "Epoch 3/100\n",
      "110/110 [==============================] - 4s 32ms/step - loss: 0.5882 - accuracy: 0.6954 - val_loss: 0.5988 - val_accuracy: 0.6853\n",
      "Epoch 4/100\n",
      "110/110 [==============================] - 3s 32ms/step - loss: 0.5454 - accuracy: 0.7280 - val_loss: 0.5911 - val_accuracy: 0.6800\n",
      "Epoch 5/100\n",
      "110/110 [==============================] - 3s 32ms/step - loss: 0.5241 - accuracy: 0.7483 - val_loss: 0.6195 - val_accuracy: 0.6840\n",
      "Epoch 6/100\n",
      "110/110 [==============================] - 4s 32ms/step - loss: 0.5016 - accuracy: 0.7586 - val_loss: 0.5485 - val_accuracy: 0.7227\n",
      "Epoch 7/100\n",
      "110/110 [==============================] - 4s 33ms/step - loss: 0.4818 - accuracy: 0.7731 - val_loss: 0.5478 - val_accuracy: 0.7367\n",
      "Epoch 8/100\n",
      "110/110 [==============================] - 3s 32ms/step - loss: 0.4787 - accuracy: 0.7709 - val_loss: 0.5298 - val_accuracy: 0.7380\n",
      "Epoch 9/100\n",
      "110/110 [==============================] - 4s 33ms/step - loss: 0.4664 - accuracy: 0.7906 - val_loss: 0.5711 - val_accuracy: 0.7253\n",
      "Epoch 10/100\n",
      "110/110 [==============================] - 4s 32ms/step - loss: 0.4584 - accuracy: 0.7929 - val_loss: 0.5664 - val_accuracy: 0.7240\n",
      "Epoch 11/100\n",
      "110/110 [==============================] - 4s 32ms/step - loss: 0.4437 - accuracy: 0.8003 - val_loss: 0.5122 - val_accuracy: 0.7513\n",
      "Epoch 12/100\n",
      "110/110 [==============================] - 4s 32ms/step - loss: 0.4318 - accuracy: 0.8029 - val_loss: 0.5171 - val_accuracy: 0.7587\n",
      "Epoch 13/100\n",
      "110/110 [==============================] - 4s 32ms/step - loss: 0.4235 - accuracy: 0.8131 - val_loss: 0.5626 - val_accuracy: 0.7273\n",
      "Epoch 14/100\n",
      "110/110 [==============================] - 4s 32ms/step - loss: 0.4019 - accuracy: 0.8269 - val_loss: 0.5544 - val_accuracy: 0.7500\n",
      "Epoch 15/100\n",
      "110/110 [==============================] - 4s 32ms/step - loss: 0.3973 - accuracy: 0.8277 - val_loss: 0.5146 - val_accuracy: 0.7613\n",
      "Epoch 16/100\n",
      "110/110 [==============================] - 4s 32ms/step - loss: 0.3964 - accuracy: 0.8229 - val_loss: 0.5344 - val_accuracy: 0.7480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2871d12a0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Fiting the model on embedded and padded data with the early stopping criterion.\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train_pad, y_train, \n",
    "          batch_size = 32,\n",
    "          epochs=100,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "26e4350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy evaluated on the test set is of 76.280%\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the test set.\n",
    "\n",
    "result = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {result[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bc1025",
   "metadata": {},
   "source": [
    "## Trained Word2Vec - Transfer Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc86488",
   "metadata": {},
   "source": [
    "### The accuracy of the above the baseline model, might be quite low. By improving the quality of the embedding we can Improve accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e8fd7a",
   "metadata": {},
   "source": [
    "### Let's improve the quality of our embedding, instead of just loading a larger corpus, let's benefit from the embedding that others have learned. Because, the quality of an embedding, i.e. the proximity of the words, can be derived from different tasks. This is exactly what transfer learning is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6734ca92",
   "metadata": {},
   "source": [
    "### Listing all the different models available in the word2vec using gensim api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b07b77ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "print(list(api.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bd25af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's load one of the pre-trained word2vec embedding spaces. \n",
    "\n",
    "word2vec_transfer = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "062fa47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(word2vec_transfer.key_to_index))\n",
    "print(len(word2vec_transfer['dog']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bc0a78fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed_2 = embedding(word2vec_transfer, X_train)\n",
    "X_test_embed_2 = embedding(word2vec_transfer, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9270ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the training and test embedded sentences\n",
    "X_train_pad_2 = pad_sequences(X_train_embed_2, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad_2 = pad_sequences(X_test_embed_2, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cdee2b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "110/110 [==============================] - 7s 41ms/step - loss: 0.6766 - accuracy: 0.5774 - val_loss: 0.6399 - val_accuracy: 0.6513\n",
      "Epoch 2/30\n",
      "110/110 [==============================] - 4s 34ms/step - loss: 0.6259 - accuracy: 0.6606 - val_loss: 0.6478 - val_accuracy: 0.6253\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - 4s 35ms/step - loss: 0.5878 - accuracy: 0.6994 - val_loss: 0.6211 - val_accuracy: 0.6567\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - 4s 35ms/step - loss: 0.5421 - accuracy: 0.7366 - val_loss: 0.5511 - val_accuracy: 0.7327\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - 4s 34ms/step - loss: 0.5047 - accuracy: 0.7589 - val_loss: 0.5931 - val_accuracy: 0.6993\n",
      "Epoch 6/30\n",
      "110/110 [==============================] - 4s 35ms/step - loss: 0.4745 - accuracy: 0.7803 - val_loss: 0.5050 - val_accuracy: 0.7647\n",
      "Epoch 7/30\n",
      "110/110 [==============================] - 4s 34ms/step - loss: 0.4507 - accuracy: 0.7983 - val_loss: 0.4906 - val_accuracy: 0.7687\n",
      "Epoch 8/30\n",
      "110/110 [==============================] - 4s 34ms/step - loss: 0.4276 - accuracy: 0.8071 - val_loss: 0.5175 - val_accuracy: 0.7400\n",
      "Epoch 9/30\n",
      "110/110 [==============================] - 4s 35ms/step - loss: 0.4100 - accuracy: 0.8200 - val_loss: 0.5833 - val_accuracy: 0.7367\n",
      "Epoch 10/30\n",
      "110/110 [==============================] - 4s 34ms/step - loss: 0.3996 - accuracy: 0.8260 - val_loss: 0.5131 - val_accuracy: 0.7753\n",
      "Epoch 11/30\n",
      "110/110 [==============================] - 4s 35ms/step - loss: 0.3814 - accuracy: 0.8354 - val_loss: 0.4741 - val_accuracy: 0.7927\n",
      "Epoch 12/30\n",
      "110/110 [==============================] - 4s 35ms/step - loss: 0.3646 - accuracy: 0.8386 - val_loss: 0.4713 - val_accuracy: 0.7927\n",
      "Epoch 13/30\n",
      "110/110 [==============================] - 4s 35ms/step - loss: 0.3533 - accuracy: 0.8514 - val_loss: 0.5279 - val_accuracy: 0.7493\n",
      "Epoch 14/30\n",
      "110/110 [==============================] - 4s 35ms/step - loss: 0.3345 - accuracy: 0.8594 - val_loss: 0.4716 - val_accuracy: 0.7973\n",
      "Epoch 15/30\n",
      "110/110 [==============================] - 4s 34ms/step - loss: 0.3195 - accuracy: 0.8654 - val_loss: 0.4791 - val_accuracy: 0.7873\n",
      "Epoch 16/30\n",
      "110/110 [==============================] - 4s 35ms/step - loss: 0.3067 - accuracy: 0.8726 - val_loss: 0.4691 - val_accuracy: 0.8013\n",
      "Epoch 17/30\n",
      "110/110 [==============================] - 4s 35ms/step - loss: 0.2944 - accuracy: 0.8786 - val_loss: 0.5124 - val_accuracy: 0.7800\n",
      "Epoch 18/30\n",
      "110/110 [==============================] - 4s 37ms/step - loss: 0.2802 - accuracy: 0.8889 - val_loss: 0.4814 - val_accuracy: 0.7980\n",
      "Epoch 19/30\n",
      "110/110 [==============================] - 4s 39ms/step - loss: 0.2661 - accuracy: 0.8937 - val_loss: 0.5344 - val_accuracy: 0.7853\n",
      "Epoch 20/30\n",
      "110/110 [==============================] - 4s 36ms/step - loss: 0.2514 - accuracy: 0.8971 - val_loss: 0.5232 - val_accuracy: 0.7807\n",
      "Epoch 21/30\n",
      "110/110 [==============================] - 4s 36ms/step - loss: 0.2430 - accuracy: 0.9066 - val_loss: 0.5403 - val_accuracy: 0.7940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28717c070>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model = init_model()\n",
    "\n",
    "model.fit(X_train_pad_2, y_train, \n",
    "          batch_size = 32,\n",
    "          epochs=30,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d8297abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy evaluated on the test set is of 80.580%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test_pad_2, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {result[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c2888",
   "metadata": {},
   "source": [
    "### There is a significant improvement in the accuracy after Transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1949d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add Sentiment_Analysis_using_NLP.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9de36bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\r\n",
      "Your branch is ahead of 'origin/master' by 1 commit.\r\n",
      "  (use \"git push\" to publish your local commits)\r\n",
      "\r\n",
      "Untracked files:\r\n",
      "  (use \"git add <file>...\" to include in what will be committed)\r\n",
      "\t\u001b[31m.DS_Store\u001b[m\r\n",
      "\t\u001b[31mmy_model/\u001b[m\r\n",
      "\t\u001b[31msaved_model.pb\u001b[m\r\n",
      "\t\u001b[31m~/\u001b[m\r\n",
      "\r\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m 'Sentiment Analysis using NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3b4f286a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating objects: 5, done.\n",
      "Counting objects: 100% (5/5), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (3/3), 2.28 KiB | 2.28 MiB/s, done.\n",
      "Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
      "To github.com:pavankumarhm/Sentiment-Analysis-Assesment.git\n",
      "   febec41..2242c44  master -> master\n",
      "branch 'master' set up to track 'origin/master'.\n"
     ]
    }
   ],
   "source": [
    "!git push -u origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c119c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
