{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a0cd07",
   "metadata": {},
   "source": [
    "# Text Sentiment Analysis of IMDB Movie reviews using NLP (Word2Vec and RNN) for MYM Intern Assesment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca3e39",
   "metadata": {},
   "source": [
    "## Assesment Objectives: \n",
    "### - Preprocessing the data\n",
    "### - Converting Text(words) to Vectors using word2vec \n",
    "### - Using the word representations given by word2vec to feed a RNN and training the model\n",
    "### - Testing the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84781652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim==4.2.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep gensim ##Checking the version of Gensim - Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e2dbaa",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b25eb77",
   "metadata": {},
   "source": [
    "### Starting with 30% of the sentences from TensorFlow Datasets of IMDB reviews to check the RAM compatibility of the PC to train the model faster by splitting the datasets as X_train, y_train, X_test and y_test.\n",
    "### Then preprocessing the textual data to create input features for a natural language processing (NLP) model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2079b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "    \n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "      assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "        \n",
    "      len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "      train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "  \n",
    "      len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "      test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "    \n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "10d21172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626f4c4c",
   "metadata": {},
   "source": [
    "## First, training a word2vec model (with the arguments that we want) on your training sentence. Store it into the `word2vec` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5c2e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(sentences=X_train, vector_size=60, min_count=10, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "62a835d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            embedded_sentence.append(word2vec.wv[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed = embedding(word2vec, X_train)\n",
    "X_test_embed = embedding(word2vec, X_test)\n",
    "\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2678ec1b",
   "metadata": {},
   "source": [
    "### It's a good practice to check check the following for `X_train_pad` and `X_test_pad`:\n",
    "#### - they are numpy arrays\n",
    "#### - they are 3-dimensional\n",
    "#### - the last dimension is of the size of your word2vec embedding space (you can get it with `word2vec.wv.vector_size`\\\\\n",
    "#### - the first dimension is of the size of your `X_train` and `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d4770855",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X in [X_train_pad, X_test_pad]:\n",
    "    assert type(X) == np.ndarray\n",
    "    assert X.shape[-1] == word2vec.wv.vector_size\n",
    "\n",
    "\n",
    "assert X_train_pad.shape[0] == len(X_train)\n",
    "assert X_test_pad.shape[0] == len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7fddad",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3b477d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels in train set {0: 3739, 1: 3761}\n",
      "Baseline accuracy:  0.49173333333333336\n"
     ]
    }
   ],
   "source": [
    "# It is always good to have a very simple model to test your own model against\n",
    "# Baseline accuracy can be to predict the label that is the most present in `y_train`.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "counts = dict(zip(unique, counts))\n",
    "print('Number of labels in train set', counts)\n",
    "\n",
    "y_pred = 0 if counts[0] > counts[1] else 1\n",
    "\n",
    "print('Baseline accuracy: ', accuracy_score(y_test, [y_pred]*len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec10def3",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "523cd8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# writing a RNN model with Masking, LSTM and Dense layers.\n",
    "\n",
    "def init_model():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Masking())\n",
    "    model.add(layers.LSTM(20, activation='tanh'))\n",
    "    model.add(layers.Dense(15, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', #compiling the model with rmsprop optimizer\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5317ce64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "165/165 [==============================] - 10s 45ms/step - loss: 0.6769 - accuracy: 0.5598 - val_loss: 0.6495 - val_accuracy: 0.6378\n",
      "Epoch 2/100\n",
      "165/165 [==============================] - 5s 32ms/step - loss: 0.6083 - accuracy: 0.6653 - val_loss: 0.5855 - val_accuracy: 0.6916\n",
      "Epoch 3/100\n",
      "165/165 [==============================] - 5s 32ms/step - loss: 0.5348 - accuracy: 0.7326 - val_loss: 0.5118 - val_accuracy: 0.7498\n",
      "Epoch 4/100\n",
      "165/165 [==============================] - 5s 32ms/step - loss: 0.4917 - accuracy: 0.7684 - val_loss: 0.5167 - val_accuracy: 0.7542\n",
      "Epoch 5/100\n",
      "165/165 [==============================] - 5s 32ms/step - loss: 0.4614 - accuracy: 0.7810 - val_loss: 0.4722 - val_accuracy: 0.7956\n",
      "Epoch 6/100\n",
      "165/165 [==============================] - 5s 32ms/step - loss: 0.4503 - accuracy: 0.7870 - val_loss: 0.4606 - val_accuracy: 0.7867\n",
      "Epoch 7/100\n",
      "165/165 [==============================] - 5s 33ms/step - loss: 0.4242 - accuracy: 0.8070 - val_loss: 0.5197 - val_accuracy: 0.7573\n",
      "Epoch 8/100\n",
      "165/165 [==============================] - 5s 32ms/step - loss: 0.4093 - accuracy: 0.8116 - val_loss: 0.4883 - val_accuracy: 0.7804\n",
      "Epoch 9/100\n",
      "165/165 [==============================] - 5s 32ms/step - loss: 0.3994 - accuracy: 0.8219 - val_loss: 0.4857 - val_accuracy: 0.7787\n",
      "Epoch 10/100\n",
      "165/165 [==============================] - 5s 32ms/step - loss: 0.3827 - accuracy: 0.8297 - val_loss: 0.4521 - val_accuracy: 0.7951\n",
      "Epoch 11/100\n",
      "165/165 [==============================] - 5s 32ms/step - loss: 0.3712 - accuracy: 0.8371 - val_loss: 0.4528 - val_accuracy: 0.8022\n",
      "Epoch 12/100\n",
      "165/165 [==============================] - 5s 32ms/step - loss: 0.3601 - accuracy: 0.8392 - val_loss: 0.4564 - val_accuracy: 0.7929\n",
      "Epoch 13/100\n",
      "165/165 [==============================] - 5s 33ms/step - loss: 0.3454 - accuracy: 0.8514 - val_loss: 0.4497 - val_accuracy: 0.8084\n",
      "Epoch 14/100\n",
      "165/165 [==============================] - 5s 32ms/step - loss: 0.3331 - accuracy: 0.8577 - val_loss: 0.4571 - val_accuracy: 0.8067\n",
      "Epoch 15/100\n",
      "165/165 [==============================] - 5s 32ms/step - loss: 0.3185 - accuracy: 0.8653 - val_loss: 0.5184 - val_accuracy: 0.7853\n",
      "Epoch 16/100\n",
      "165/165 [==============================] - 5s 32ms/step - loss: 0.3102 - accuracy: 0.8665 - val_loss: 0.5872 - val_accuracy: 0.7622\n",
      "Epoch 17/100\n",
      "165/165 [==============================] - 5s 32ms/step - loss: 0.2999 - accuracy: 0.8745 - val_loss: 0.4749 - val_accuracy: 0.7978\n",
      "Epoch 18/100\n",
      "165/165 [==============================] - 5s 33ms/step - loss: 0.2877 - accuracy: 0.8806 - val_loss: 0.7272 - val_accuracy: 0.7173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17e6b34f0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Fiting the model on embedded and padded data with the early stopping criterion.\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train_pad, y_train, \n",
    "          batch_size = 32,\n",
    "          epochs=100,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "26e4350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy evaluated on the test set is of 80.427%\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the test set.\n",
    "\n",
    "result = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {result[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443a2d48",
   "metadata": {},
   "source": [
    "## Trained Word2Vec - Transfer Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857795c2",
   "metadata": {},
   "source": [
    "### The accuracy of the above the baseline model, might be quite low. By improving the quality of the embedding we can Improve accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e82c3d9",
   "metadata": {},
   "source": [
    "### Let's improve the quality of our embedding, instead of just loading a larger corpus, let's benefit from the embedding that others have learned. Because, the quality of an embedding, i.e. the proximity of the words, can be derived from different tasks. This is exactly what transfer learning is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bb7de6",
   "metadata": {},
   "source": [
    "### Listing all the different models available in the word2vec using gensim api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b07b77ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "print(list(api.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bd25af6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "word2vec_transfer = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "062fa47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(word2vec_transfer.key_to_index))\n",
    "print(len(word2vec_transfer['dog']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bc0a78fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed_2 = embedding(word2vec_transfer, X_train)\n",
    "X_test_embed_2 = embedding(word2vec_transfer, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9270ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the training and test embedded sentences\n",
    "X_train_pad_2 = pad_sequences(X_train_embed_2, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad_2 = pad_sequences(X_test_embed_2, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdee2b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "165/165 [==============================] - 9s 39ms/step - loss: 0.6701 - accuracy: 0.5792 - val_loss: 0.6811 - val_accuracy: 0.5671\n",
      "Epoch 2/10\n",
      "165/165 [==============================] - 6s 34ms/step - loss: 0.6194 - accuracy: 0.6568 - val_loss: 0.7026 - val_accuracy: 0.5778\n",
      "Epoch 3/10\n",
      "165/165 [==============================] - 6s 34ms/step - loss: 0.5673 - accuracy: 0.7110 - val_loss: 0.5895 - val_accuracy: 0.6907\n",
      "Epoch 4/10\n",
      "165/165 [==============================] - 6s 34ms/step - loss: 0.5250 - accuracy: 0.7505 - val_loss: 0.6206 - val_accuracy: 0.6889\n",
      "Epoch 5/10\n",
      "165/165 [==============================] - 6s 34ms/step - loss: 0.5015 - accuracy: 0.7653 - val_loss: 0.4784 - val_accuracy: 0.7844\n",
      "Epoch 6/10\n",
      "165/165 [==============================] - 6s 35ms/step - loss: 0.4792 - accuracy: 0.7766 - val_loss: 0.4622 - val_accuracy: 0.7902\n",
      "Epoch 7/10\n",
      "165/165 [==============================] - 6s 34ms/step - loss: 0.4462 - accuracy: 0.8011 - val_loss: 0.5971 - val_accuracy: 0.7102\n",
      "Epoch 8/10\n",
      "165/165 [==============================] - 6s 35ms/step - loss: 0.4333 - accuracy: 0.8048 - val_loss: 0.5463 - val_accuracy: 0.7413\n",
      "Epoch 9/10\n",
      " 77/165 [=============>................] - ETA: 2s - loss: 0.4222 - accuracy: 0.8093"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model = init_model()\n",
    "\n",
    "model.fit(X_train_pad_2, y_train, \n",
    "          batch_size = 32,\n",
    "          epochs=10,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )\n",
    "model.save('my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8297abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy evaluated on the test set is of 78.213%\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(X_test_pad_2, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1949d6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de36bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a1393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
