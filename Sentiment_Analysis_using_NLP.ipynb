{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a0cd07",
   "metadata": {},
   "source": [
    "# Text Sentiment Analysis of IMDB Movie reviews using NLP (Word2Vec and RNN) for MYM Intern Assesment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca3e39",
   "metadata": {},
   "source": [
    "## Assesment Objectives: \n",
    "### - Preprocessing the data\n",
    "### - Converting Text(words) to Vectors using word2vec \n",
    "### - Using the word representations given by word2vec to feed a RNN and training the model\n",
    "### - Evaluating the model and plotting the performance graphs\n",
    "### - Improving the model by Transfer Learning\n",
    "### - Comparing Accuracy of Baseline model, The model and Improved model.\n",
    "### - Testing the model (predicting the model with new review)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",

   "id": "84781652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim==4.2.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep gensim ##Checking the version of Gensim - Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e2dbaa",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b25eb77",
   "metadata": {},
   "source": [
    "### Starting with 20% of the sentences from TensorFlow Datasets of IMDB reviews to check the RAM compatibility of the PC to train the model faster by splitting the datasets as X_train, y_train, X_test and y_test.\n",
    "### Then preprocessing the textual data to create input features for a natural language processing (NLP) model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",

   "id": "2079b965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 10:34:53.357804: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "    \n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "      assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "        \n",
    "      len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "      train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "  \n",
    "      len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "      test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "    \n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=20)"
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 1, 0, 0])"
      ]
     },

     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",

   "metadata": {},
   "source": [
    "## First, training a word2vec model (with the arguments that we want) on your training sentence. Store it into the `word2vec` variable. "
   ]
  },
  {
   "cell_type": "code",

   "id": "f5c2e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(sentences=X_train, vector_size=60, min_count=10, window=10)\n",
    "word2vec.save(\"word2vec.model\")\n",
    "\n"

   ]
  },
  {
   "cell_type": "code",

   "id": "62a835d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            embedded_sentence.append(word2vec.wv[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed = embedding(word2vec, X_train)\n",
    "X_test_embed = embedding(word2vec, X_test)\n",
    "\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "markdown",

   "metadata": {},
   "source": [
    "### It's a good practice to check check the following for `X_train_pad` and `X_test_pad`:\n",
    "#### - they are numpy arrays\n",
    "#### - they are 3-dimensional\n",
    "#### - the last dimension is of the size of your word2vec embedding space (you can get it with `word2vec.wv.vector_size`\\\\\n",
    "#### - the first dimension is of the size of your `X_train` and `X_test`"
   ]
  },
  {
   "cell_type": "code",

   "id": "d4770855",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X in [X_train_pad, X_test_pad]:\n",
    "    assert type(X) == np.ndarray\n",
    "    assert X.shape[-1] == word2vec.wv.vector_size\n",
    "\n",
    "\n",
    "assert X_train_pad.shape[0] == len(X_train)\n",
    "assert X_test_pad.shape[0] == len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",

   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",

   "id": "3b477d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels in train set {0: 2474, 1: 2526}\n",
      "Baseline accuracy:  0.499\n"
     ]
    }
   ],
   "source": [
    "# It is always good to have a very simple model to test your own model against\n",
    "# Baseline accuracy can be to predict the label that is the most present in `y_train`.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "counts = dict(zip(unique, counts))\n",
    "print('Number of labels in train set', counts)\n",
    "\n",
    "y_pred = 0 if counts[0] > counts[1] else 1\n",
    "\n",
    "print('Baseline accuracy: ', accuracy_score(y_test, [y_pred]*len(y_test)))\n",
    "\n",
    "baseline_acc = accuracy_score(y_test, [y_pred]*len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",

   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",

   "id": "523cd8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# writing a RNN model with Masking, LSTM and Dense layers.\n",
    "\n",
    "def init_model():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Masking())\n",
    "    model.add(layers.LSTM(20, activation='tanh'))\n",
    "    model.add(layers.Dense(15, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', #compiling the model with rmsprop optimizer\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = init_model()"
   ]
  },
  {
   "cell_type": "code",

   "id": "5317ce64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",

     ]
    }
   ],
   "source": [
    "#  Fiting the model on embedded and padded data with the early stopping criterion.\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train_pad, y_train, \n",
    "          batch_size = 32,\n",
    "          epochs=100,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )\n",
    "the_model_acc = history.history['accuracy'][-1]\n"
   ]
  },
  {
   "cell_type": "code",

   "id": "26e4350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [

     ]
    }
   ],
   "source": [
    "# Evaluating the model on the test set.\n",
    "\n",
    "result = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {result[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [
    {
     "data": {

      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the accuracy and loss curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",

   "metadata": {},
   "source": [
    "## Trained Word2Vec - Transfer Learning\n"
   ]
  },
  {
   "cell_type": "markdown",

   "metadata": {},
   "source": [
    "### The accuracy of the above the baseline model, might be quite low. By improving the quality of the embedding we can Improve accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",

   "metadata": {},
   "source": [
    "### Let's improve the quality of our embedding, instead of just loading a larger corpus, let's benefit from the embedding that others have learned. Because, the quality of an embedding, i.e. the proximity of the words, can be derived from different tasks. This is exactly what transfer learning is."
   ]
  },
  {
   "cell_type": "markdown",

   "metadata": {},
   "source": [
    "### Listing all the different models available in the word2vec using gensim api."
   ]
  },
  {
   "cell_type": "code",

   "id": "b07b77ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "print(list(api.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",

   "id": "bd25af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's load one of the pre-trained word2vec embedding spaces. \n",
    "\n",
    "word2vec_transfer = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",

   "id": "062fa47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(word2vec_transfer.key_to_index))\n",
    "print(len(word2vec_transfer['dog']))"
   ]
  },
  {
   "cell_type": "code",

   "id": "bc0a78fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed_2 = embedding(word2vec_transfer, X_train)\n",
    "X_test_embed_2 = embedding(word2vec_transfer, X_test)"
   ]
  },
  {
   "cell_type": "code",

   "id": "9270ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the training and test embedded sentences\n",
    "X_train_pad_2 = pad_sequences(X_train_embed_2, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad_2 = pad_sequences(X_test_embed_2, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "code",

   "id": "cdee2b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",

     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model = init_model()\n",
    "\n",
    "history = model.fit(X_train_pad_2, y_train, \n",
    "          batch_size = 32,\n",
    "          epochs=30,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )\n",

   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d8297abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [

     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test_pad_2, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {result[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [
    {
     "data": {

      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the accuracy and loss curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",

   "metadata": {},
   "source": [
    "### There is a significant improvement in the accuracy after Transfer learning."
   ]
  },
  {
   "cell_type": "markdown",

   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [

     ]
    }
   ],
   "source": [

   ]
  },
  {
   "cell_type": "code",

   "id": "1949d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add Sentiment_Analysis_using_NLP.ipynb"
   ]
  },
  {
   "cell_type": "code",

   "id": "9de36bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [

     ]
    }
   ],
   "source": [
    "!git commit -m 'Sentiment Analysis using NLP'"
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [

     ]
    }
   ],
   "source": [

   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
